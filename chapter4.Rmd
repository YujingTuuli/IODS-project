---
title: "chapter4"
output: html_document
date: "2023-11-26"
---
# Chapter 4: Clustering and classification

```{r}
date()
```

## 1 Load the Boston data

*Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. *

```{r}
library(MASS)
data("Boston")
str(Boston)
dim(Boston)
```

The data set is about housing values in suburbs of Boston. There are 506 observations of 14 variables, including numeric and integer variables. The 14 variables respectively refer to:
1. crim: per capita crime rate by town.
2. zn: proportion of residential land zoned for lots over 25,000 sq.ft.
3. indus: proportion of non-retail business acres per town.
4. chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
5. nox: nitrogen oxides concentration (parts per 10 million).
6. rm: average number of rooms per dwelling.
7. age: proportion of owner-occupied units built prior to 1940.
8. dis: weighted mean of distances to five Boston employment centres.
9. rad: index of accessibility to radial highways.
10. tax: full-value property-tax rate per $10,000.
11. ptratio: pupil-teacher ratio by town.
12. black: 1000(Bkâˆ’0.63)2 where Bk is the proportion of blacks by town.
13. lstat: lower status of the population (percent).
14. medv: median value of owner-occupied homes in $1000s.

## 2 Graphical overview

*Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them.*

```{r}
# Show summaries of the variables
summary(Boston)

# First make a distribution plot for 14 variables
library(ggplot2)
library(tidyr)

# Convert the data to long format for easier plotting
long_boston <- gather(Boston)

# Plotting
p1 <- ggplot(long_boston, aes(x = value)) +
  geom_density(fill = "skyblue", color = "black") +
  facet_wrap(~key, scales = "free") +
  theme_minimal() +
  labs(title = "Overview of Boston dataset")

# Print the plot
p1

# Then make a correlations plot to look at the correlations among the 14 variables in the data
library(corrplot)

# Calculate the correlation matrix and round it
cor_matrix <- cor(Boston) |> 
  round(digits = 2)

# Print the correlation matrix
print(cor_matrix)

# Visualize the correlation matrix
corrplot(cor_matrix, method="circle", type = "upper")

```
From the distribution plot, most of the variables are skewed to right or left direction. Only rm is realatively normally distributed. From the correlation matrix, the strongest correlations are between the variables rad and tax (positive), dis and age (negative), dis and nox (negative), dis and indus (negative), lstat and medv (negative).


## 3 Standardize the dataset; create a categorical variable of the crime rate; divide the dataset to train and test sets

*Standardize the dataset and print out summaries of the scaled data. How did the variables change? Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set.*


### 3.1 Standardize the dataset
```{r}
# Center and standardize variables
boston_scaled <- scale(Boston) |> 
  as.data.frame()

# Summaries of the scaled variables
summary(boston_scaled)
```

After scaling the data, all the means of the variables turn to zero. Most of the values of the variables ranged from -4 and 4, only except for crim (crime rate).

### 3.2 Create a categorical variable of the crime rate

```{r}
# Summary of the scaled crime rate
summary(bosten_scaled$crim)

# Create a quantile vector of crim
bins <- quantile(bosten_scaled$crim)

# Create a categorical variable 'crime'
crime <- cut(bosten_scaled$crim, breaks = bins, include.lowest = TRUE,
             labels = c("low", "med_low","med_high", "high"))

# Look at the table of the new factor crime
table(crime)

# Remove original crim from the dataset
boston_scaled <- dplyr::select(bosten_scaled, -crim)

# Add the new categorical value to scaled data
boston_scaled <- data.frame(bosten_scaled, crime)
```

### 3.3 Divide the dataset to train and test sets

```{r}
library(dplyr)

# Number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# Choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# Create train set
train <- boston_scaled[ind,]

# Create test set 
test <- boston_scaled[-ind,]

```

## 4 Linear discriminant analysis

*Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot.*

```{r}
# Fit the linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# Print the lda.fit object
lda.fit

# Draw the LDA (bi)plot
# The function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# Target classes as numeric
classes <- as.numeric(train$crime)

# Plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```
Biplot based on LD1 and LD2 was generated. From the LDA biplot, the clusters of low, med_low, and med_high classes separate poorly. Heavy overlap was observed between these three clusters. The cluster of high class separates well. But the clusters high and med_iumH_high also showed notable overlaps. Based on arrows, varaibles rad explained the most for cluster of high class. Contributions of variables to other clusters are not clear enough due to the heavy overlap.

## 5 Predict LDA

*Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results.*

```{r}
# Save the correct classes from test data
correct_classes <- test$crime

# Remove the crime variable from test data
test <- dplyr::select(test, -crime)

# Predict classes with the LDA model on the test data
lda.pred <- predict(lda.fit, newdata = test)

# Cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

The cross tabulated results show that most of the predictions on the classes of med_low, med_high, and high are correct. But the prediction of the low class has just only less than a half correctness. This result shows a not so satisfactory predicting effect of our linear discriminant analysis.

## 6 Distance measure

*Reload the Boston dataset and standardize the dataset (we did not do this in the Exercise Set, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results.*

### 6.1 Reload the Boston dataset and standardize the dataset

```{r}
data("Boston")
boston_scaled <- scale(Boston) |> 
  as.data.frame()
```

### 6.2 Calculate the distances between the observations

```{r}
# Euclidean distances matrix
dist_eu <- dist(boston_scaled)

# Look at the summary of the distances
summary(dist_eu)
```

### 6.3 Determine K-means for the optimal number of clusters

```{r}
set.seed(123)
k_max <- 10

# Calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# Visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```
The optimal number of clusters is when the value of total WCSS changes radically. In this case, two clusters would seem optimal.

### 6.4 Visualize the clusters and interpret the results

```{r fig.height= 6, fig.width=10}
# K-means clustering with 2 clusters
km <- kmeans(boston_scaled, centers = 2)

# Plot the scaled Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
pairs(boston_scaled[1:7], col = km$cluster)
pairs(boston_scaled[8:14], col = km$cluster)
```
Most of the variables are influential linear separators for the clusters, except rad, ptratio, and tax.

## 7 Super-Bonus: 3D plot

*Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points. Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities?*

```{r}
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
# Install and access the plotly package. Create a 3D plot (cool!) of the columns of the matrix product using the code below.
library(plotly)
p3 <- plot_ly(x = matrix_product$LD1, 
        y = matrix_product$LD2, 
        z = matrix_product$LD3, 
        color = train$crime, #Set the color to be the crime classes of the train set. 
        type= 'scatter3d', 
        mode='markers',
        size = 2)

# Draw another 3D plot where the color is defined by the clusters of the k-means
# Get the clusters of k-means for the train set
train.km <- kmeans(model_predictors, centers = 2) 

p4 <- plot_ly(x = matrix_product$LD1, 
        y = matrix_product$LD2, 
        z = matrix_product$LD3, 
        type= 'scatter3d', 
        mode='markers', 
        color = factor(train.km$cluster), #color defined by clusters of the k-means
        size = 2)

```
The LDA was trained according to a mathematical category of crime rates (quantiles), which has four categories. While k = 2 was adopted for the k-means clustering base on the size of within-cluster sum of square. Since LDA is a supervised technique, we know what are each categories represent, which are also labeled in the caption. K-means clustering is a unsupervised method and thus I do not know anything about the real-world representation of the 2 clusters identified before observing closely. However, by observing the 3D plots together, it is interesting to find out that, cluster 2 from k-means nicely overlaps with high category from LDA. Also, cluster 1 from k-means roughly overlaps with the other three categories from LDA.








