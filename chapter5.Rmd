---
title: "chapter5"
output: html_document
date: "2023-11-30"
---

# Chapter 5: Dimensionality reduction techniques

```{r}
date()
```

## 1 Overview of the data

*Move the country names to rownames (see Exercise 5.5). Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them.*

### 1.1 Read the data

```{r}
library(tidyverse)

human <- read_csv("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/human2.csv")
```

### 1.2 Move the country names to rownames

```{r}
human_1 <- column_to_rownames(human, "Country")

summary(human_1)
```

### 1.3 Graphical overview

```{r fig.height= 6, fig.width=10}
library(GGally)

# For making a nicer plot, creat some functions
# Define a function that allows me for more control over ggpairs. This function produces point plot with fitted lines.
my.fun.smooth <- function(data,    # my function needs 3 arguments
                          mapping,
                          method = "lm"){
  ggplot(data = data, # data is passed from ggpairs' arguments
         mapping = mapping)+ # aes is passed from ggpairs' arguments
           geom_point(size = 0.3,  # draw points
                      color = "black")+
           geom_smooth(method = method,  # fit a linear regression
                       size = 0.3, 
                       color = "red")+
           theme(panel.grid.major = element_blank(), # get rid of the grids
                 panel.grid.minor = element_blank(),
                 panel.background = element_rect(fill = "wheat", #adjust background
                                                 color = "black"))
} 

# Define a function that allows me for more control over ggpairs. Tthis function produces density plot.
my.fun.density <- function(data, mapping, ...) { # notes are roughly same with above

    ggplot(data = data, mapping = mapping) +
       geom_histogram(aes(y=..density..),
                      color = "black", 
                      fill = "white")+
       geom_density(fill = "#FF6666", alpha = 0.25) +
       theme(panel.grid.major = element_blank(), 
             panel.grid.minor = element_blank(),
             panel.background = element_rect(fill = "lightblue",
                                             color = "black"))
} 

ggpairs(human_1, #data
        progress = FALSE,
        lower = 
          list(continuous = my.fun.smooth), # lower half show points with fitted line
        diag =
          list(continuous = my.fun.density), # diagonal grids show density plots
        title = "Relationships between variables") + # title
  theme (plot.title = element_text(size = 22,  # adjust title visuals
                                   face = "bold"))
```
Only the expected years of education (Edu.Exp) is normally distributed. The rest of variables are skewed in one way or another. Most of the variables have strong correlation with other variables, except Parli.F and Labo.F.

## 2 Principal component analysis (PCA) and biplot

*Perform principal component analysis (PCA) on the raw (non-standardized) human data. Show the variability captured by the principal components. Draw a biplot displaying the observations by the first two principal components (PC1 coordinate in x-axis, PC2 coordinate in y-axis), along with arrows representing the original variables.*

### 2.1 PCA

```{r}
# Perform principal component analysis on the raw (non-standardized) human data
pca_human <- prcomp(human_1)
print(pca_human)
summary(pca_human)
```
PC1 explains 99.99% of the variability of the data set. Other components’ contribution is less the 0.1% in totality.

### 2.2 Draw a biplot

```{r fig.height=8, fig.width=8}
biplot_1 <- biplot(pca_human, choices = 1:2,
       cex = c(0.8, 1),
       col = c("grey40", "deeppink2"))
```
In the biplot, red texts are variables, and grey texts are observations (countries). The position of GNI is far away from the origin (0,0) in the direction of x axis (PC1), indicating its strong contribution to PC1. Most of the countries clustered tightly around the origin (0,0), which indicates that they are not well-represented on the factor map.

## 3 Repeat PCA with standardized data and biplot

*Standardize the variables in the human data and repeat the above analysis. Interpret the results of both analysis (with and without standardizing). Are the results different? Why or why not? Include captions (brief descriptions) in your plots where you describe the results by using not just your variable names, but the actual phenomena they relate to.*

### 3.1 Standardize the variables in the human data

```{r}
human_std <- scale(human_1)
summary(human_std)
```

### 3.2 Perform PCA again

```{r}
pca_human_std <- prcomp(human_std)
summary(pca_human_std)
```

After standardizing, PC1 explains 53.6% of the variability of the data set. PC2 explains the other 16%. PC1 and PC2 together can explain about 70% of the variability of the data set.

### 3.3 Biplot

```{r fig.height=8, fig.width=8}
biplot_2 <- biplot(pca_human_std, choices = 1:2,
       cex = c(0.8, 1),
       col = c("grey40", "deeppink2"))
```

After standardizing data set, row and column points are more well scattered across the coordinate panel and all the variables are visualized more reasonably. The scattered country names are well-represented in the factor map.

## 4 Personal interpretations of two PCAs and biplots

*Give your personal interpretations of the first two principal component dimensions based on the biplot drawn after PCA on the standardized human data.*

Base on finding above, it is not hard to draw the conclusion that PCA using standardized data set produces results better for analysis. Possible explanation for this is the variables with different scales make the comparison between pairs of features difficult. PCA is calculated based on co-variance. Unlike correlation which is dimensionless, covariance is in units obtained by multiplying the units of the two variables. When data set is not scaled, this makes each variable not easily comparable with others (since they all have their own value ranges). Further, each variable loads almost exclusively on one components because they can hardly find another variable with comparable value range. This assumption is further consolidated by the fact that the only two variables with smaller loading scores are Edu2.FM and Labo.FM, both of which happen to have similar value range from 0 to 1. Also, co-variance also gives some variable extremely high leverage in our data set. Look back to the summary of the raw human data, “GNI” has a scale tremendously larger than other variables. This might lead to its large co-variances with any other variable, and further results in its over-contribution to the factor solution. All of these mis-representation of data would end up the poor quality of contribution, and hence the biplot shows most of the countries clustered tightly together, indicating the PCA has not produced a factor map with acceptable dissimilarity among rows. Also, the over-contribution of GNI to the factor solution leads to a graph with only one variable–GNI–showing in a visible distance (others overlap heavily around the center).

## 5 Tea data

*The tea data comes from the FactoMineR package and it is measured with a questionnaire on tea: 300 individuals were asked how they drink tea (18 questions) and what are their product's perception (12 questions). In addition, some personal details were asked (4 questions).*

### 5.1 Load the tea dataset and convert its character variables to factors

```{r}
# Load the data
library(FactoMineR)
data(tea)
str(tea)
dim(tea)

# Convert its character variables to factors
# According to the structure, the characters are already factors.
```
### 5.2 Multiple Correspondence Analysis (MCA)

*Use Multiple Correspondence Analysis (MCA) on the tea data (or on just certain columns of the data, it is up to you!). Interpret the results of the MCA and draw at least the variable biplot of the analysis. You can also explore other plotting options for MCA. Comment on the output of the plots.*

```{r}
# Select columns
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- select(tea, one_of(keep_columns))
summary(tea_time)
str(tea_time)

# Visualize the dataset
library(ggplot2)
pivot_longer(tea_time, cols = everything()) %>% 
  ggplot(aes(value)) + facet_wrap("name", scales = "free")+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))

# MCA
mca <- MCA(tea_time, graph = FALSE)
summary(mca)

# Visualize MCA
plot(mca, invisible=c("ind"), graph.type = "classic",habillage = "quali")
```
### 5.3 Interpretation

Multiple Correspondence Analysis (MCA) is a multivariate statistical technique used for analyzing the relationships between categorical variables. MCA results are often presented in terms of principal components. 

Eigenvalues in the summary represent the amount of variance captured by each principal component. Higher eigenvalues indicate more important dimensions. The two most important dimensions explain respectively 15.238% and 14.232% variables.

The Plot is a variable plot which examines the dispersion of categories in six variables along the two principal components. Categories close to each other are positively associated, while those far apart are negatively associated. And the relationships between categories can be seen from their degree of proximity. The positions of the categories on the principal components can indicate the contribution of the dimension. For example, sugar and no suger contribute little to both dimensions, while enjoying at tea shop and unpackaged tea contibute greatly in the first dimension. 



